import cv2
import time
import pandas as pd
import numpy as np
from datetime import datetime
import os
from typing import List, Dict
import queue
import threading

from utils.config import load_config
from utils.logger import setup_logger
from utils.visualizer import Visualizer
from utils.image_utils import apply_perspective_correction, preprocess_for_ocr
from drivers.camera import Camera
from core.detector import ContainerDetector
from services.ocr_worker import ContainerOCR

# --- ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ë¥¼ ìœ„í•œ ì›Œì»¤ ìŠ¤ë ˆë“œ í•¨ìˆ˜ ---
def ocr_processing_thread(task_queue, ocr_worker, logger, history, system_conf):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ OCR ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ìš”ë¦¬ì‚¬ (Consumer)
    """
    logger.info("ğŸ‘¨â€ğŸ³ OCR ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì‹œì‘ë¨")
    
    while True:
        try:
            # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸° (ë¸”ë¡œí‚¹ ëª¨ë“œ)
            task = task_queue.get()
            
            if task is None: # ì¢…ë£Œ ì‹ í˜¸
                break
                
            unit_name = task['unit_name']
            track_id = task['track_id']
            images = task['images'] # [{'img':..., 'path':...}, ...]
            
            logger.info(f"ğŸ³ [OCR] ì²˜ë¦¬ ì‹œì‘: {unit_name} ID:{track_id} (ì´ë¯¸ì§€ {len(images)}ì¥)")
            
            # ë””ìŠ¤í¬ì— ì €ì¥ (Qwen ì…ë ¥ìš©)
            img_paths = []
            for item in images:
                # ì´ë¯¸ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì“°ê³ , ì—†ìœ¼ë©´ ì €ì¥ (ë©”ëª¨ë¦¬ ë²„í¼ì¸ ê²½ìš°)
                if 'path' in item and os.path.exists(item['path']):
                    img_paths.append(item['path'])
                else:
                    # í˜¹ì‹œ ì €ì¥ ì•ˆ ëœ ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ì €ì¥
                    pass 

            # OCR ìˆ˜í–‰ (Batch)
            if img_paths:
                res = ocr_worker.process_batch(img_paths)
                verdict = ocr_worker.consolidate_results(res)
                
                if verdict['found']:
                    num = verdict['container_number']
                    logger.info(f"ğŸ‰ [ê²°ê³¼] {unit_name} ID:{track_id} -> í™•ì •: {num}")
                    
                    # ê²°ê³¼ ê¸°ë¡ (Thread-safe í•˜ê²Œ append)
                    record = {
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'unit': unit_name,
                        'track_id': track_id,
                        'number': num,
                        'confidence_votes': f"{verdict.get('voting_meta', {}).get('winner_count')}/{len(img_paths)}"
                    }
                    history.append(record)
                    
                    # CSV ì¦‰ì‹œ ì €ì¥ (ì˜µì…˜)
                    log_path = system_conf.get('log_file', 'outputs/gate_log.csv')
                    # íŒŒì¼ I/OëŠ” ëŠë¦¬ë¯€ë¡œ ì‹¤ì œ ìƒìš©ì—ì„  DBë‚˜ ë³„ë„ ë¡œê±° ì‚¬ìš© ê¶Œì¥
                    # ì—¬ê¸°ì„  í¸ì˜ìƒ ë®ì–´ì“°ê¸°ë³´ë‹¤ëŠ” append ëª¨ë“œë¡œ ì—¬ëŠ” ê²Œ ì¢‹ìœ¼ë‚˜, ê¸°ì¡´ ë¡œì§ ìœ ì§€
                    
                else:
                    logger.info(f"ğŸ’¨ [ì‹¤íŒ¨] {unit_name} ID:{track_id} -> ì¸ì‹ ë¶ˆê°€")
            
            # ì‘ì—… ì™„ë£Œ ì‹ í˜¸
            task_queue.task_done()
            
        except Exception as e:
            logger.error(f"ğŸ”¥ OCR ì›Œì»¤ ì—ëŸ¬: {e}")
            task_queue.task_done()

def calculate_complex_score(image, conf, box_area, frame_area):
    """
    ë³µí•© ì ìˆ˜ ê³„ì‚° (ì„ ëª…ë„ + í™•ì‹  + í¬ê¸°)
    """
    # 1. ì„ ëª…ë„ (0~ìˆ˜ì²œ) -> ì •ê·œí™” í•„ìš”í•˜ì§€ë§Œ ìƒëŒ€ ë¹„êµìš©ìœ¼ë¡œ ì”€
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
    
    # 2. í¬ê¸° ë¹„ìœ¨ (0.0 ~ 1.0)
    size_ratio = box_area / (frame_area + 1e-5)
    
    # ê°€ì¤‘ì¹˜ í•©ì‚° (heuristic)
    # ì„ ëª…ë„ê°€ ì œì¼ ì¤‘ìš”í•˜ì§€ë§Œ, ë„ˆë¬´ ì‘ê±°ë‚˜ í™•ì‹  ë‚®ì€ ê±´ ê±°ë¦„
    score = (sharpness * 0.5) + (conf * 1000 * 0.3) + (size_ratio * 10000 * 0.2)
    return score, sharpness

def resize_frame(frame, scale=0.5):
    return cv2.resize(frame, None, fx=scale, fy=scale)

def main():
    config = load_config()
    system_conf = config.get('system', {})
    model_conf = config.get('model', {})
    params_conf = config.get('parameters', {})
    
    logger = setup_logger(log_file=system_conf.get('log_file', 'outputs/gate_log.csv'))
    logger.info("=== ë¹„ë™ê¸°(Async) ìŠ¤ë§ˆíŠ¸ íŠ¸ë˜í‚¹ ì‹œìŠ¤í…œ ì‹œì‘ ===")

    # 1. ì´ˆê¸°í™”
    camera_units = [] 
    camera_configs = system_conf.get('cameras', [])
    global_target_classes = model_conf.get('target_classes', None)
    
    try:
        # OCR ëª¨ë¸ ë¡œë“œ
        ocr_worker = ContainerOCR(model_name=model_conf.get('ocr_model', 'Qwen/Qwen3-VL-2B-Instruct'))
        
        # ì¹´ë©”ë¼ ìœ ë‹› ìƒì„±
        for conf in camera_configs:
            name = conf.get('name', 'unknown')
            src = conf.get('source')
            weights = conf.get('weights')
            zone = conf.get('detection_zone', {'x_min': 0.4, 'x_max': 0.6, 'y_min': 0.25, 'y_max': 0.75})
            target_classes = conf.get('target_classes', global_target_classes)
            
            if not src: continue
            try:
                cam = Camera(src)
                detector = ContainerDetector(
                    model_path=weights,
                    default_model=model_conf.get('yolo_default', 'yolo11n.pt'),
                    conf_threshold=model_conf.get('conf_threshold', 0.5)
                )
                camera_units.append({
                    'cam': cam, 'detector': detector, 'name': name,
                    'fps': cam.fps, 'acc': 0.0,
                    'zone': zone, 'target_classes': target_classes,
                    'track_buffer': {} 
                })
                logger.info(f"âœ… ìœ ë‹›: {name} | Targets: {target_classes}")
            except Exception as e:
                logger.error(f"âŒ ìœ ë‹› ì‹¤íŒ¨ ({name}): {e}")

        if not camera_units: return
        min_fps = min(u['fps'] for u in camera_units)

    except Exception as e:
        logger.error(f"ì´ˆê¸°í™” ì—ëŸ¬: {e}")
        return

    # 2. ë¹„ë™ê¸° í & ì›Œì»¤ ì„¤ì •
    task_queue = queue.Queue() # ë¬´í•œ í¬ê¸° í (ë©”ëª¨ë¦¬ ì£¼ì˜)
    history = []
    
    # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘ (Daemonìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ë©”ì¸ ì¢…ë£Œ ì‹œ ìë™ ì¢…ë£Œ)
    worker_thread = threading.Thread(
        target=ocr_processing_thread,
        args=(task_queue, ocr_worker, logger, history, system_conf),
        daemon=True
    )
    worker_thread.start()

    # íŒŒë¼ë¯¸í„°
    perspective_intensity = params_conf.get('perspective_intensity', 0.0)
    MAX_BUFFER_SIZE = 5      
    TRACK_PATIENCE = 1.0     # ì´ˆ ë‹¨ìœ„
    
    temp_dir = system_conf.get('temp_frame_dir', 'temp_frames')
    os.makedirs(temp_dir, exist_ok=True)

    global_step = 0

    logger.info(">>> ë©”ì¸ ë£¨í”„ ì‹œì‘ (ì¹´ë©”ë¼ëŠ” ë©ˆì¶”ì§€ ì•ŠëŠ”ë‹¤)")

    while True:
        global_step += 1
        active_frames = []
        all_closed = True
        
        # --- í”„ë ˆì„ ì½ê¸° ---
        for unit in camera_units:
            unit['acc'] += (unit['fps'] / min_fps)
            num_to_read = int(unit['acc'])
            unit['acc'] -= num_to_read
            
            frame = None
            for _ in range(num_to_read):
                f = unit['cam'].get_frame()
                if f is not None: frame = f
            
            if frame is not None:
                active_frames.append((frame, unit))
                all_closed = False
            else:
                active_frames.append((None, unit))

        if all_closed: 
            # íì— ë‚¨ì€ ì‘ì—… ë‹¤ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°í•˜ê³  ì‹¶ìœ¼ë©´ task_queue.join() ì‚¬ìš©
            break

        # --- ë©”ì¸ ì²˜ë¦¬ ë£¨í”„ (Non-Blocking) ---
        display_frames = []
        current_time = time.time()

        for frame, unit in active_frames:
            if frame is None: continue
            
            disp = frame.copy()
            fh, fw = frame.shape[:2]
            frame_area = fh * fw
            zone = unit['zone']
            buffer = unit['track_buffer']
            
            # ì¡´ í‘œì‹œ
            zx1, zx2 = int(fw * zone['x_min']), int(fw * zone['x_max'])
            zy1, zy2 = int(fh * zone['y_min']), int(fh * zone['y_max'])
            cv2.rectangle(disp, (zx1, zy1), (zx2, zy2), (255, 200, 0), 2)
            
            # --- Tracking ---
            results = unit['detector'].track(frame)
            
            if results and results[0].boxes.id is not None:
                boxes = results[0].boxes
                
                for box, track_id in zip(boxes, boxes.id):
                    tid = int(track_id)
                    cls_id = int(box.cls[0])
                    conf = float(box.conf[0])
                    
                    if unit['target_classes'] and cls_id not in unit['target_classes']:
                        continue
                        
                    x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                    cx, cy = (x1+x2)//2, (y1+y2)//2
                    box_area = (x2-x1) * (y2-y1)
                    
                    is_centered = (zx1 < cx < zx2) and (zy1 < cy < zy2)
                    
                    color = (0, 255, 0) if is_centered else (0, 0, 255)
                    cv2.rectangle(disp, (x1, y1), (x2, y2), color, 2)
                    cv2.putText(disp, f"ID:{tid}", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                    
                    if is_centered:
                        # ë²„í¼ ê´€ë¦¬
                        if tid not in buffer:
                            buffer[tid] = {'images': [], 'last_seen': current_time, 'enqueued': False}
                        
                        buffer[tid]['last_seen'] = current_time
                        
                        # ì „ì²˜ë¦¬ & ì ìˆ˜ ê³„ì‚°
                        pad = 20
                        crop = frame[max(0, y1-pad):min(fh, y2+pad), max(0, x1-pad):min(fw, x2+pad)].copy()
                        pre = preprocess_for_ocr(crop)
                        final_img = apply_perspective_correction(pre, intensity=perspective_intensity)
                        
                        score, sharpness = calculate_complex_score(final_img, conf, box_area, frame_area)
                        
                        # íŒŒì¼ ê²½ë¡œ ë¯¸ë¦¬ ìƒì„± (ë‚˜ì¤‘ì— ì“°ê¸° ìœ„í•´)
                        img_path = os.path.join(temp_dir, f"{unit['name']}_ID{tid}_{global_step}.jpg")
                        
                        img_entry = {'score': score, 'sharpness': sharpness, 'path': img_path, 'img': final_img}
                        
                        # Aì»· ê²½ìŸ (Top-K ìœ ì§€)
                        stored = buffer[tid]['images']
                        if len(stored) < MAX_BUFFER_SIZE:
                            # ë””ìŠ¤í¬ ì“°ê¸° (ì—¬ê¸°ì„œ ì“°ë©´ ì•½ê°„ ëŠë ¤ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ì›Œì»¤ ë¶€ë‹´ ì¤„ì„)
                            cv2.imwrite(img_path, final_img)
                            stored.append(img_entry)
                        else:
                            # ê¼´ë“± ì°¾ê¸° (ì ìˆ˜ ê¸°ì¤€)
                            min_score_idx = min(range(len(stored)), key=lambda i: stored[i]['score'])
                            if score > stored[min_score_idx]['score']:
                                # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
                                try: os.remove(stored[min_score_idx]['path']) 
                                except: pass
                                
                                # ìƒˆ íŒŒì¼ ì“°ê¸° & êµì²´
                                cv2.imwrite(img_path, final_img)
                                stored[min_score_idx] = img_entry
                        
                        cv2.putText(disp, f"Sc:{int(score)}", (x1, y2+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

            # --- í‡´ì¥ ì²´í¬ & í ì „ì†¡ ---
            ids_to_remove = []
            for tid, data in buffer.items():
                if data['enqueued']: continue
                
                # ì‚¬ë¼ì§„ ì§€ ì˜¤ë˜ëìœ¼ë©´
                if current_time - data['last_seen'] > TRACK_PATIENCE:
                    if data['images']:
                        # â˜… íì— ì‘ì—… ë˜ì§€ê¸° (Non-blocking)
                        logger.info(f"ğŸš€ [ì „ì†¡] {unit['name']} ID:{tid} -> OCR í ({len(data['images'])}ì¥)")
                        
                        task = {
                            'unit_name': unit['name'],
                            'track_id': tid,
                            'images': data['images'] # [{'path':...}, ...]
                        }
                        task_queue.put(task)
                    
                    data['enqueued'] = True
                    ids_to_remove.append(tid)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            for tid in list(buffer.keys()):
                if buffer[tid]['enqueued'] and (current_time - buffer[tid]['last_seen'] > TRACK_PATIENCE * 2):
                    del buffer[tid]

            # í ìƒíƒœ í‘œì‹œ
            q_size = task_queue.qsize()
            cv2.putText(disp, f"OCR Queue: {q_size}", (10, fh-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
            display_frames.append(resize_frame(disp, scale=0.4))

        if display_frames:
            combined = np.hstack(display_frames)
            cv2.imshow('Async System', combined)

        if cv2.waitKey(1) & 0xFF == ord('q'): break

    # ì¢…ë£Œ ì²˜ë¦¬
    logger.info("ğŸ›‘ ì‹œìŠ¤í…œ ì¢…ë£Œ ìš”ì²­. ì”ì—¬ ì‘ì—… ì²˜ë¦¬ ì¤‘...")
    task_queue.put(None) # ì›Œì»¤ ì¢…ë£Œ ì‹ í˜¸
    worker_thread.join() # ì›Œì»¤ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
    
    for unit in camera_units: unit['cam'].release()
    cv2.destroyAllWindows()
    
    if history:
        pd.DataFrame(history).to_csv(system_conf.get('log_file', 'outputs/gate_log.csv'), index=False)
        logger.info("ğŸ’¾ ë¡œê·¸ ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    main()
