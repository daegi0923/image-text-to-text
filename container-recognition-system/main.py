import cv2
import time
import pandas as pd
import numpy as np
from datetime import datetime
import os
from typing import List, Dict
import queue
import threading
from collections import deque

from utils.config import load_config
from utils.logger import setup_logger
from utils.image_utils import apply_perspective_correction, preprocess_for_ocr
from drivers.camera import Camera
from core.detector import ContainerDetector
from services.ocr_worker import ContainerOCR

# --- [ì‹œìŠ¤í…œ ìƒíƒœ ê´€ë¦¬ì] ---
class TriggerManager:
    def __init__(self, duration=5.0):
        self.active = False
        self.last_trigger_time = 0
        self.duration = duration
        self.trigger_source = None

    def activate(self, source_name="unknown"):
        self.active = True
        self.last_trigger_time = time.time()
        self.trigger_source = source_name

    def update(self):
        if self.active and (time.time() - self.last_trigger_time > self.duration):
            self.active = False
            self.trigger_source = None
            return False # Deactivated just now
        return self.active

    def is_active(self):
        return self.active

# --- [ì„¸ì…˜ ê´€ë¦¬ì] ê²°ê³¼ ì§‘ê³„ ---
class GateSessionManager:
    def __init__(self, logger, log_file, timeout=10.0):
        self.logger = logger
        self.log_file = log_file
        self.timeout = timeout 
        self.current_session = [] 
        self.last_update_time = 0
        self.session_start_time = 0
        self.is_session_active = False
        self.ref_image_path = None # Masterê°€ ì°ì€ ì „ì²´ ìƒ·

    def notify_trigger(self, image_path):
        """íŠ¸ë¦¬ê±° ë°œìƒ ì‹œ í˜¸ì¶œ (Masterê°€ ì°ì€ ì‚¬ì§„ ì ‘ìˆ˜)"""
        if not self.is_session_active:
            self.session_start_time = time.time()
            self.is_session_active = True
            self.ref_image_path = image_path # ì„¸ì…˜ ëŒ€í‘œ ì´ë¯¸ì§€ ì €ì¥
            self.logger.info("ğŸ¬ [ì„¸ì…˜ ì‹œì‘] íŠ¸ëŸ­ ì§„ì… ê°ì§€")
        self.last_update_time = time.time()

    def add_result(self, result_data):
        self.current_session.append(result_data)
        self.last_update_time = time.time()
        self.logger.info(f"ğŸ“¥ [ìˆ˜ì§‘] {result_data['number']} (Cam:{result_data['unit']}) - ëˆ„ì  {len(self.current_session)}ê±´")

    def update(self):
        # ë°ì´í„°ê°€ ë“¤ì–´ì˜¨ ì§€ ì˜¤ë˜ëìœ¼ë©´ ì„¸ì…˜ ì¢…ë£Œ
        if self.is_session_active and (time.time() - self.last_update_time > self.timeout):
            self.finalize_session()

    def finalize_session(self):
        if not self.is_session_active:
            return
        
        duration = time.time() - self.session_start_time
        
        # [Case 1] ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìŒ -> ë¹ˆ íŠ¸ëŸ­ or ì¸ì‹ ì‹¤íŒ¨
        if not self.current_session:
            self.logger.warning(f"âš ï¸ [ë¯¸íƒì§€] ë¬¼ì²´ëŠ” ì§€ë‚˜ê°”ìœ¼ë‚˜ ë²ˆí˜¸ ì¸ì‹ ì‹¤íŒ¨ (ì†Œìš”: {duration:.2f}ì´ˆ)")
            
            record = {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'final_number': "EMPTY_OR_FAILED",
                'vote_count': 0,
                'total_samples': 0,
                'units': "Master_Only",
                'duration_sec': round(duration, 2),
                'evidence_img_1': self.ref_image_path, # Top ë·° ì‚¬ì§„ ì €ì¥
                'evidence_img_2': None,
                'evidence_img_3': None,
            }
        else:
            # [Case 2] ì •ìƒ ì¸ì‹
            vote_box = {}
            for item in self.current_session:
                num = item['number']
                vote_box[num] = vote_box.get(num, 0) + 1 

            sorted_votes = sorted(vote_box.items(), key=lambda x: x[1], reverse=True)
            winner_num, votes = sorted_votes[0]
            units_involved = list(set([item['unit'] for item in self.current_session]))

            # í•„í„°ë§ (ë„ˆë¬´ ì§§ê±°ë‚˜ ì ìœ¼ë©´ ë¬´ì‹œí•˜ë˜, ë¡œê·¸ì—ëŠ” ë‚¨ê¸°ê³  ì‹¶ë‹¤ë©´ ë¡œì§ ì¡°ì • ê°€ëŠ¥)
            # ì—¬ê¸°ì„œëŠ” 7ìë¦¬ ë¯¸ë§Œë„ ì¼ë‹¨ ê¸°ë¡í•˜ë˜ ê²½ê³ ë§Œ ì°ìŒ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜: ë¹ˆ íŠ¸ëŸ­ë„ ë‚¨ê²¨ì•¼ í•˜ë¯€ë¡œ)
            
            # ì¦ê±° ì‚¬ì§„ ì„ ë°œ
            all_evidence = []
            for item in self.current_session:
                if item['number'] == winner_num:
                    if 'evidence_images' in item:
                        all_evidence.extend(item['evidence_images'])
            all_evidence.sort(key=lambda x: x.get('score', 0), reverse=True)
            top_3 = all_evidence[:3]

            self.logger.info(f"ğŸ† [í™•ì •] {winner_num} (íˆ¬í‘œ: {votes}/{len(self.current_session)})")

            record = {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'final_number': winner_num,
                'vote_count': votes,
                'total_samples': len(self.current_session),
                'units': ",".join(units_involved),
                'duration_sec': round(duration, 2),
                'evidence_img_1': top_3[0]['path'] if len(top_3) > 0 else self.ref_image_path,
                'evidence_img_2': top_3[1]['path'] if len(top_3) > 1 else None,
                'evidence_img_3': top_3[2]['path'] if len(top_3) > 2 else None,
            }
        
        # íŒŒì¼ ì €ì¥
        df = pd.DataFrame([record])
        header = not os.path.exists(self.log_file)
        df.to_csv(self.log_file, mode='a', header=header, index=False, encoding='utf-8-sig')
        
        # ì´ˆê¸°í™”
        self.current_session = []
        self.is_session_active = False
        self.ref_image_path = None


# --- [OCR ì›Œì»¤] GPU ì¼ê´„ ì²˜ë¦¬ ---
def ocr_global_batch_worker(task_queue, result_queue, ocr_worker, logger):
    logger.info("ğŸ‘¨â€ğŸ³ [OCR Worker] ëŒ€ê¸° ì¤‘...")
    
    pending_tasks = []
    accumulated_images = []
    accumulated_meta = []
    
    last_batch_time = time.time()
    MAX_BATCH_SIZE = 16
    BATCH_TIMEOUT = 0.5

    while True:
        try:
            task = task_queue.get(timeout=0.1)
            if task is None: break 
            
            valid_imgs = [img for img in task['images'] if os.path.exists(img['path'])]
            if valid_imgs:
                pending_tasks.append(task)
                for img in valid_imgs:
                    accumulated_images.append(img['path'])
                    accumulated_meta.append({
                        'task_idx': len(pending_tasks) - 1
                    })
            task_queue.task_done()
        except queue.Empty:
            pass

        # ë°°ì¹˜ ì‹¤í–‰ ì¡°ê±´
        is_full = len(accumulated_images) >= MAX_BATCH_SIZE
        is_timeout = (len(accumulated_images) > 0) and (time.time() - last_batch_time > BATCH_TIMEOUT)
        
        if is_full or is_timeout:
            try:
                # GPU Inference
                results = ocr_worker.process_batch(accumulated_images)
                
                # ê²°ê³¼ ì¬ë¶„ë°°
                task_results_map = {i: [] for i in range(len(pending_tasks))}
                for i, res in enumerate(results):
                    t_idx = accumulated_meta[i]['task_idx']
                    task_results_map[t_idx].append(res)

                # ê° Taskë³„ ê²°ê³¼ ì§‘ê³„
                for idx, task in enumerate(pending_tasks):
                    verdict = ocr_worker.consolidate_results(task_results_map[idx])
                    if verdict['found']:
                        result_queue.put({
                            'unit': task['unit_name'],
                            'number': verdict['container_number'],
                            'track_id': task['track_id'],
                            'evidence_images': task['images'] # [ì¤‘ìš”] ì›ë³¸ ì´ë¯¸ì§€ ì •ë³´ ì „ë‹¬
                        })
                    else:
                        pass 
            except Exception as e:
                logger.error(f"OCR Batch Error: {e}")
            
            # Reset
            pending_tasks = []
            accumulated_images = []
            accumulated_meta = []
            last_batch_time = time.time()


# --- [ìœ í‹¸] ì ìˆ˜ ê³„ì‚° ---
def calculate_score(image, conf, box_area):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
    # ì„ ëª…ë„ + ì‹ ë¢°ë„ + í¬ê¸°(ê°€ê¹Œìš¸ìˆ˜ë¡ í¼)
    score = (sharpness * 0.4) + (conf * 1000 * 0.4) + (box_area * 0.2)
    return score

def resize_display(frame, scale=0.4):
    return cv2.resize(frame, None, fx=scale, fy=scale)

# --- [ë©”ì¸] ---
def main():
    config = load_config()
    sys_conf = config.get('system', {})
    param_conf = config.get('parameters', {})
    
    log_file = sys_conf.get('log_file', 'outputs/gate_log.csv')
    
    # [ìˆ˜ì •] ì‹œìŠ¤í…œ ë¡œê·¸ëŠ” ë³„ë„ íŒŒì¼(system.log)ì— ì €ì¥í•˜ì—¬ CSV ì˜¤ì—¼ ë°©ì§€
    system_log_path = os.path.join(os.path.dirname(log_file), 'system.log')
    logger = setup_logger(log_file=system_log_path)
    
    logger.info("=== [Top-Triggered] ì»¨í…Œì´ë„ˆ ì¸ì‹ ì‹œìŠ¤í…œ ì‹œì‘ ===")

    # 1. ì´ˆê¸°í™”
    trigger_manager = TriggerManager(duration=param_conf.get('trigger_duration', 5.0))
    session_manager = GateSessionManager(logger, log_file)
    
    task_queue = queue.Queue()
    result_queue = queue.Queue()

    # ì¹´ë©”ë¼ ì„¤ì • ë¡œë“œ
    cameras = []
    try:
        ocr_worker = ContainerOCR(model_name=config['model'].get('ocr_model', 'paddle'))
        
        for cam_conf in sys_conf.get('cameras', []):
            name = cam_conf.get('name')
            role = cam_conf.get('role', 'slave') 
            src = cam_conf.get('source')
            if not src: continue

            try:
                cam = Camera(src)
                detector = ContainerDetector(
                    model_path=cam_conf.get('weights'),
                    conf_threshold=config['model'].get('conf_threshold', 0.5)
                )
                cameras.append({
                    'name': name, 'role': role, 'cam': cam, 'detector': detector,
                    'zone': cam_conf.get('detection_zone'),
                    'targets': cam_conf.get('target_classes'), 
                    'buffer': {}, 
                    'fps': cam.fps, 'acc': 0.0,
                    'frame_idx': 0,
                    'last_disp_frame': None # [NEW] í™”ë©´ ìœ ì§€ìš© ë§ˆì§€ë§‰ í”„ë ˆì„
                })
                logger.info(f"ğŸ¥ [{role.upper()}] {name} ì¤€ë¹„ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì¹´ë©”ë¼ ë¡œë“œ ì‹¤íŒ¨ ({name}): {e}")

        if not cameras: 
            logger.error("ì¹´ë©”ë¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    # OCR ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
    threading.Thread(target=ocr_global_batch_worker, 
                     args=(task_queue, result_queue, ocr_worker, logger), 
                     daemon=True).start()

    # í´ë” ìƒì„±
    temp_dir = sys_conf.get('temp_frame_dir', 'temp_frames')
    os.makedirs(temp_dir, exist_ok=True)

    min_fps = min(c['fps'] for c in cameras)
    logger.info(">>> ì‹œìŠ¤í…œ ë£¨í”„ ì‹œì‘ (Press 'q' to exit)")

    # 2. Main Loop
    while True:
        current_time = time.time()
        
        # (1) ì„¸ì…˜ ë° íŠ¸ë¦¬ê±° ìƒíƒœ ì—…ë°ì´íŠ¸
        while not result_queue.empty():
            session_manager.add_result(result_queue.get())
        
        session_manager.update()
        trigger_active = trigger_manager.update()

        # (2) í”„ë ˆì„ ì½ê¸° (ë™ê¸°í™”)
        active_frames = []
        all_closed = True
        
        for c in cameras:
            c['acc'] += (c['fps'] / min_fps)
            n_read = int(c['acc'])
            c['acc'] -= n_read
            
            frame = None
            for _ in range(n_read):
                f = c['cam'].get_frame()
                if f is not None: frame = f
            
            if frame is not None:
                active_frames.append((frame, c))
                all_closed = False
            else:
                active_frames.append((None, c))
        
        if all_closed: break

        # (3) ê°ì§€ ë° ë¡œì§ ìˆ˜í–‰
        display_frames = []
        
        for frame, unit in active_frames:
            # [ìˆ˜ì •] í”„ë ˆì„ì´ ì—†ìœ¼ë©´(Syncë¡œ ì¸í•´ ìŠ¤í‚µë¨) ë§ˆì§€ë§‰ í™”ë©´ ìœ ì§€
            if frame is None:
                if unit['last_disp_frame'] is not None:
                    display_frames.append(unit['last_disp_frame'])
                else:
                    # ì´ˆê¸° ìƒíƒœ: ê²€ì€ í™”ë©´ (640x360 ì˜ˆì‹œ)
                    black = np.zeros((360, 640, 3), dtype=np.uint8)
                    display_frames.append(resize_display(black))
                continue
            
            disp = frame.copy()
            role = unit['role']
            fh, fw = frame.shape[:2]
            
            # ROI í‘œì‹œ
            z = unit['zone']
            zx1, zx2 = int(fw*z['x_min']), int(fw*z['x_max'])
            zy1, zy2 = int(fh*z['y_min']), int(fh*z['y_max'])
            
            zone_color = (0, 0, 255) if (role == 'master' and trigger_active) else \
                         (255, 0, 0) if (role == 'master') else \
                         (0, 255, 0) if (role == 'slave' and trigger_active) else (100, 100, 100)
                
            cv2.rectangle(disp, (zx1, zy1), (zx2, zy2), zone_color, 2)
            cv2.putText(disp, f"{role.upper()}", (zx1, zy1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, zone_color, 2)

            # --- [CORE LOGIC] ---
            should_detect = (role == 'master') or (role == 'slave' and trigger_active)
            
            # [ìµœì í™” 1] ëª¨ë“  ì¹´ë©”ë¼(Master í¬í•¨) 3í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆë§Œ ì¶”ë¡ 
            # íŠ¸ëŸ­ì€ ë¹ ë¥´ì§€ ì•Šìœ¼ë¯€ë¡œ 30fps ê°ì‹œëŠ” ìì› ë‚­ë¹„ì„ (10fpsë©´ ì¶©ë¶„)
            unit['frame_idx'] += 1
            SKIP_INTERVAL = 3 
            if unit['frame_idx'] % SKIP_INTERVAL != 0:
                should_detect = False

            if should_detect:
                # [ìµœì í™” 2] ì¶”ë¡ ìš© ë¦¬ì‚¬ì´ì¦ˆ (640px)
                # ì›ë³¸ì´ FHDë©´ YOLO ì „ì²˜ë¦¬ê°€ ì˜¤ë˜ ê±¸ë¦¼ -> ë¯¸ë¦¬ ì¤„ì—¬ì„œ ë˜ì ¸ì¤Œ
                DETECT_W = 640
                scale_factor = fw / DETECT_W
                detect_h = int(fh / scale_factor)
                small_frame = cv2.resize(frame, (DETECT_W, detect_h))

                results = unit['detector'].track(small_frame)
                
                if results and results[0].boxes.id is not None:
                    boxes = results[0].boxes
                    for box, track_id in zip(boxes, boxes.id):
                        cls_id = int(box.cls[0])
                        
                        if unit['targets'] and cls_id not in unit['targets']:
                            continue
                        
                        # [ì¢Œí‘œ ë³µì›] ì‘ì€ ì´ë¯¸ì§€ ì¢Œí‘œ -> ì›ë³¸ ì¢Œí‘œ
                        s_x1, s_y1, s_x2, s_y2 = map(int, box.xyxy[0].cpu().numpy())
                        x1 = int(s_x1 * scale_factor)
                        y1 = int(s_y1 * scale_factor)
                        x2 = int(s_x2 * scale_factor)
                        y2 = int(s_y2 * scale_factor)

                        cx, cy = (x1+x2)//2, (y1+y2)//2
                        
                        # ROI ì²´í¬
                        if not (zx1 < cx < zx2 and zy1 < cy < zy2):
                            continue

                        cv2.rectangle(disp, (x1, y1), (x2, y2), zone_color, 2)
                        
                        # [Master]
                        if role == 'master':
                            if not trigger_active:
                                logger.info(f"ğŸ”” [TRIGGER] {unit['name']} -> ì‹œìŠ¤í…œ ê°€ë™")
                                # [NEW] íŠ¸ë¦¬ê±° ì‹œì‘ ì‹œì ì— Topë·° ì‚¬ì§„ í•œ ì¥ ë°•ì œ (ë¹ˆ íŠ¸ëŸ­ ì¦ê±°ìš©)
                                ref_img_path = os.path.join(temp_dir, f"REF_{unit['name']}_{int(current_time)}.jpg")
                                cv2.imwrite(ref_img_path, frame)
                                session_manager.notify_trigger(ref_img_path)
                            
                            trigger_manager.activate(source_name=unit['name'])
                            session_manager.notify_trigger(None) 
                            
                            # Master ê°ì§€ ì‹œê°í™”
                            cv2.rectangle(disp, (x1, y1), (x2, y2), (0, 0, 255), 2)
                            cv2.putText(disp, f"TRUCK {conf:.2f}", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

                        # [Slave]
                        elif role == 'slave':
                            tid = int(track_id)
                            conf = float(box.conf[0])
                            
                            # Slave ê°ì§€ ì‹œê°í™” (ë°•ìŠ¤ + Conf)
                            # íŠ¹íˆ Code Area(2)ëŠ” ëˆˆì— ë„ê²Œ ì´ˆë¡ìƒ‰ìœ¼ë¡œ!
                            box_color = (0, 255, 0) if cls_id == 2 else (255, 255, 255)
                            label = f"CODE {conf:.2f}" if cls_id == 2 else f"OBJ {conf:.2f}"
                            cv2.rectangle(disp, (x1, y1), (x2, y2), box_color, 2)
                            cv2.putText(disp, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)

                            # ì˜¤ì§ Code Area(2)ì¸ ê²½ìš°ë§Œ OCR ëŒ€ìƒ
                            if cls_id != 2:
                                continue

                            if tid not in unit['buffer']:
                                unit['buffer'][tid] = {'images': [], 'last_seen': 0, 'sent': False}
                            
                            buf = unit['buffer'][tid]
                            buf['last_seen'] = current_time
                            
                            # 1. ìº¡ì²˜ (ì•½ê°„ì˜ ì—¬ìœ  ê³µê°„ ì¶”ê°€)
                            pad = 15
                            crop = frame[max(0, y1-pad):min(fh, y2+pad), max(0, x1-pad):min(fw, x2+pad)]
                            
                            # 2. [ì¤‘ìš”] OCR ì „ì²˜ë¦¬ ë° ì›ê·¼ ë³´ì • ë³µêµ¬
                            # configì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
                            p_intensity = param_conf.get('perspective_intensity', 0.0)
                            preprocessed = preprocess_for_ocr(crop)
                            final_img = apply_perspective_correction(preprocessed, intensity=p_intensity)
                            
                            # 3. ì ìˆ˜ ê³„ì‚° (ë³´ì •ëœ ì´ë¯¸ì§€ ê¸°ì¤€)
                            score = calculate_score(final_img, conf, (x2-x1)*(y2-y1))
                            img_path = os.path.join(temp_dir, f"{unit['name']}_ID{tid}_{int(current_time*1000)}.jpg")
                            
                            # ë²„í¼ ê´€ë¦¬ (Top 5 ìœ ì§€)
                            if len(buf['images']) < 5: 
                                cv2.imwrite(img_path, final_img)
                                buf['images'].append({'path': img_path, 'score': score})
                            else:
                                worst = min(buf['images'], key=lambda x: x['score'])
                                if score > worst['score']:
                                    try: os.remove(worst['path'])
                                    except: pass
                                    buf['images'].remove(worst)
                                    cv2.imwrite(img_path, final_img)
                                    buf['images'].append({'path': img_path, 'score': score})

            # Slave í ì „ì†¡
            if role == 'slave':
                to_remove = []
                for tid, data in unit['buffer'].items():
                    if not data['sent'] and (current_time - data['last_seen'] > 1.0):
                        if data['images']:
                            task_queue.put({
                                'unit_name': unit['name'],
                                'track_id': tid,
                                'images': data['images']
                            })
                        data['sent'] = True
                        to_remove.append(tid)
                    elif data['sent']:
                         to_remove.append(tid)
                
                for tid in to_remove:
                    del unit['buffer'][tid]
            
            # [ìˆ˜ì •] í™”ë©´ ìºì‹± ë° ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            final_disp = resize_display(disp)
            unit['last_disp_frame'] = final_disp
            display_frames.append(final_disp)

        if display_frames:
            # 4ëŒ€ì¼ ê²½ìš° 2x2 ê²©ìë¡œ ë°°ì¹˜
            if len(display_frames) == 4:
                top_row = np.hstack(display_frames[:2])
                bot_row = np.hstack(display_frames[2:])
                combined = np.vstack([top_row, bot_row])
            else:
                combined = np.hstack(display_frames)

            status_text = f"SYSTEM: {'RECORDING' if trigger_active else 'IDLE'}"
            color = (0, 0, 255) if trigger_active else (0, 255, 0)
            cv2.putText(combined, status_text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)
            cv2.imshow('Container Recognition System', combined)

        if cv2.waitKey(1) & 0xFF == ord('q'): break

    logger.info("ğŸ›‘ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
    task_queue.put(None)
    for c in cameras: c['cam'].release()
    cv2.destroyAllWindows()
    session_manager.finalize_session()

if __name__ == "__main__":
    main()
